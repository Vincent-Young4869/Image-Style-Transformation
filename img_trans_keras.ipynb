{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"img_trans_keras.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SfdegwivCUrQ","executionInfo":{"elapsed":5466,"status":"ok","timestamp":1636148697157,"user":{"displayName":"Vincent Young","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhMQp7rcKpD4q6aD-gNzxTzQGL5plg3iiAYpYj-=s64","userId":"15913363348651917887"},"user_tz":240},"outputId":"d87dccdb-06a7-4e44-bbe1-e8bb2b53bf2b"},"source":["%tensorflow_version 1.x\n","import tensorflow as tf\n","print(tf.__version__)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["TensorFlow 1.x selected.\n","1.15.2\n"]}]},{"cell_type":"code","metadata":{"id":"oPzR94yjCU1T"},"source":["import glob\n","import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from tensorflow.keras.applications.vgg16 import VGG16\n","from tensorflow.keras.applications.vgg19 import VGG19\n","from tensorflow.keras.models import Model\n","from tensorflow.keras import backend as K\n","\n","def get_image_data(path, resize=None, dtype=np.float32):\n","    filepath = glob.glob(path)[0]\n","    image = cv2.imread(filepath).astype(dtype)\n","    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","    ratio = None\n","    if resize!=None:\n","        if isinstance(resize, (list, tuple)):\n","            resize = (resize[1], resize[0])\n","        else:\n","          assert type(resize)==int\n","          height, width, channels = image.shape\n","          w = width if width<height else height\n","          ratio = resize/w\n","          resize = (int(width*ratio), int(height*ratio))\n","        image = cv2.resize(image, resize, interpolation=cv2.INTER_CUBIC)\n","    return image, ratio\n","  \n","def preprocessing(image):\n","    image[:, :, 0] -= 123.68\n","    image[:, :, 1] -= 116.779\n","    image[:, :, 2] -= 103.939  \n","    return image\n","  \n","  \n","def deprocessing(image):\n","    image[:, :, 0] += 123.68\n","    image[:, :, 1] += 116.779\n","    image[:, :, 2] += 103.939   \n","    image = np.clip(image, 0, 255).astype('uint8')   \n","    return image\n","  \n","\n","def Gram_Matrix(x):\n","    assert len(x.shape) == 3, \"wrong shape of input tensor, should be: (height, width, n_channel)\"   \n","    # flatten the imgae tensor\n","    x = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n","    # compute the gram matrix, shape : (n_channel, n_channel)\n","    gram = K.dot(x, K.transpose(x))\n","\n","    return gram\n","\n","def content_loss_fcn(content, generated):\n","    return 1/2 * tf.reduce_sum(tf.pow((generated - content), 2))\n","\n","def style_loss_fcn(style, generated, style_weight):\n","    gram_style = Gram_Matrix(style)\n","    gram_generated = Gram_Matrix(generated)                   # Gram Matrix of Generated feature vector\n","    \n","    height, width, n_channel = generated.shape\n","    size = height.value * width.value\n","    norm_factor = 1 / (4 * (n_channel.value ** 2) * (size ** 2))\n","    \n","    return style_weight * norm_factor * tf.reduce_sum(tf.pow((gram_generated - gram_style), 2))\n","\n","  \n","def total_variation_loss(x):\n","    assert len(x.shape)==4, \"wrong shape for input of total variation loss, need to fix\"\n","    _, height, width, _ = x.shape\n","    \n","    height = height.value\n","    width  = width.value\n","  \n","    a = tf.square(x[:, :height-1, :width-1, :] - x[:, 1:, :width-1, :])\n","    b = tf.square(x[:, :height-1, :width-1, :] - x[:, :height-1, 1:, :])\n","    \n","    return tf.reduce_sum(tf.pow(a + b, 1.25))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RG3k6yEhKP4V","executionInfo":{"elapsed":23214,"status":"ok","timestamp":1636150422636,"user":{"displayName":"Vincent Young","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhMQp7rcKpD4q6aD-gNzxTzQGL5plg3iiAYpYj-=s64","userId":"15913363348651917887"},"user_tz":240},"outputId":"859dc94e-73a9-4c8c-f509-5915cc6639d6"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"0qJEnvEqCVAC","outputId":"c8b5ab63-6cb2-45cf-cca7-df1181d49f5d"},"source":["if __name__ == '__main__':\n","\n","    # parameters for general model settings:\n","    content_img_path    = '/content/drive/My Drive/Colab Notebooks/Image Style Transformation/target_img/gate.jpg'\n","    style_img_path      = '/content/drive/My Drive/Colab Notebooks/Image Style Transformation/style_img/style5.jpg'\n","    image_resize        = 512\n","    rescale_image       = False\n","    content_blocks      = ['block4_conv2']\n","    style_blocks        = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1']\n","    style_weights       = {block : 1/len(style_blocks) for block in style_blocks}\n","    loss_ratio          = 1e-3\n","    tv_weight           = 5e-8\n","    model_type          = 0         # 0 == vgg16, 1 == vgg19\n","    optimizer_type      = 0         # Options are 0 == Adam Optimizer, 1 == L-BFGS-B Optimizer.\n","\n","    # hyper parameter for Adam Optimizer:\n","    learning_rate       = 1e+1\n","    beta_1              = 0.8\n","    beta_2              = 0.999\n","    epsilon             = 1e-08\n","    iteration           = 150\n","\n","    # Load content and style images, initialize generated image with random noise\n","    content_image, rescale = get_image_data(content_img_path, image_resize)\n","    style_image, _         = get_image_data(style_img_path, content_image.shape[:2])\n","    print(\"Image Shape: \", content_image.shape)\n","    \n","    generated_image = tf.random.normal(shape=content_image.shape, stddev=np.std(content_image))\n","    generated_image = tf.Variable(generated_image, dtype=tf.float32, name='random_noise', trainable=True)\n","\n","\n","    # images preprocessing\n","    content_image = preprocessing(content_image)\n","    style_image   = preprocessing(style_image)\n","\n","    image_shape = (1,) + content_image.shape  # shape = (1, height, width, 3)\n","    \n","    content_image   = content_image.reshape(image_shape)\n","    style_image     = style_image.reshape(image_shape)\n","    init_tensor    = tf.reshape(generated_image, shape=image_shape)\n","\n","\n","    # Load pretrained model\n","    with tf.compat.v1.variable_scope('pretrained_model'):\n","      if model_type == 0:\n","        model = VGG16(weights='imagenet', input_tensor=init_tensor, include_top=False)\n","      elif model_type == 1:\n","        model = VGG19(weights='imagenet', input_tensor=init_tensor, include_top=False)\n","        \n","      keras_variables = [var.name for var in tf.global_variables() if 'pretrained_model' in var.name]\n","    \n","    output_dict = {layer.name: layer.output for layer in model.layers}\n","        \n","\n","    # Session\n","    sess = K.get_session() \n","    K.set_session(sess)\n","\n","    content_features = {}\n","    style_features = {}\n","    \n","    for block in content_blocks:\n","      feature_maps = sess.run(output_dict[block], feed_dict={init_tensor: content_image})[0]\n","      content_features[block] = tf.constant(feature_maps, dtype=tf.float32)\n","\n","    for block in style_blocks:\n","      feature_maps = sess.run(output_dict[block], feed_dict={init_tensor: style_image})[0]\n","      style_features[block] = tf.constant(feature_maps, dtype=tf.float32)#Gram_Matrix(feature_maps)\n","    \n","    # Content Loss\n","    content_loss = 0\n","    for block in content_blocks:\n","      generated = output_dict[block][0] \n","      content = content_features[block]\n","      content_loss +=  content_loss_fcn(content, generated)\n","    \n","    # Style Loss \n","    style_loss = 0\n","    for block in style_blocks:\n","      generated = output_dict[block][0]            \n","      style = style_features[block]                        # Gram Matrix of Style feature vector\n","      w = style_weights[block]\n","      style_loss += style_loss_fcn(style, generated, w)\n","      \n","    # Total Variation Loss\n","    tv_loss = tv_weight * total_variation_loss(init_tensor)\n","      \n","    loss = loss_ratio * content_loss + style_loss + tv_loss\n","\n","\n","    # Minimize cost with optimizers\n","    trainble_variables = [var for var in tf.global_variables() if 'pretrained_model' not in var.name]  # Should not train the weights of pretrained model.\n","    if optimizer_type == 0:\n","        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=beta_1, beta2=beta_2, epsilon=epsilon).minimize(loss, var_list=trainble_variables)\n","    elif optimizer_type == 1:\n","        optimizer = tf.contrib.opt.ScipyOptimizerInterface(loss, var_list=trainble_variables, method='L-BFGS-B', options={'maxiter': iteration})\n","\n","\n","    # Initialize\n","    uninitialize_variables = [var for var in tf.global_variables() if var.name not in keras_variables]\n","    sess.run(tf.variables_initializer(uninitialize_variables))\n","\n","    #show_weights_histogram(model)\n","     \n","    # Training\n","    with sess.as_default():\n","      if optimizer_type == 0:  # Adam Optimizer\n","        for i in range(iteration):  \n","            _cost, _c_cost, _s_cost, _tv_cost, _ = sess.run([loss, content_loss, style_loss, tv_loss, optimizer])\n","\n","            if i % ((iteration // 10)) == 0:\n","                print('iter : {}'.format(i + 1), 'total loss : {:.2f}'.format(_cost),\n","                      'content_loss : {:.2f}'.format(_c_cost), 'style_loss : {:.2f}'.format(_s_cost))\n","      if optimizer_type == 1:  # L-BFGS-B Optimizer \n","        _iter = 0\n","        def callback(_cost, _c_cost, _s_cost, _tv_loss):\n","            global _iter\n","            if _iter % ((iteration // 10)) == 0:\n","              print('iter : {}'.format(_iter + 1), 'total loss : {:.2f}'.format(_cost),\n","                    'content_loss : {:.2f}'.format(_c_cost), 'style_loss : {:.2f}'.format(_s_cost),\n","                     'tv_loss : {:.2f}'.format(_tv_loss))\n","            _iter += 1\n","\n","        optimizer.minimize(sess, fetches=[loss, content_loss, style_loss, tv_loss], loss_callback=callback)\n","        \n","      \n","    print(\"training complete.\")\n","    \n","    generated_image = sess.run(init_tensor)[0]\n","    generated_image = deprocessing(generated_image)\n","    \n","    if rescale_image == True:\n","      generated_image = cv2.resize(generated_image, None, fx=1/rescale, fy=1/rescale, interpolation=cv2.INTER_CUBIC)\n","    \n","    print(\"Final Image Shape =\", generated_image.shape)\n","    \n","    K.clear_session()"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Image Shape:  (512, 682, 3)\n","iter : 1 total loss : 9112537088.00 content_loss : 86548234240.00 style_loss : 9025968128.00\n","iter : 16 total loss : 118495224.00 content_loss : 54845386752.00 style_loss : 63632376.00\n","iter : 31 total loss : 75959584.00 content_loss : 44230250496.00 style_loss : 31711924.00\n","iter : 46 total loss : 59497320.00 content_loss : 35081568256.00 style_loss : 24398340.00\n","iter : 61 total loss : 50466924.00 content_loss : 29716180992.00 style_loss : 20733308.00\n","iter : 76 total loss : 44658384.00 content_loss : 26252421120.00 style_loss : 18388498.00\n","iter : 91 total loss : 40513892.00 content_loss : 23792375808.00 style_loss : 16704020.00\n","iter : 106 total loss : 37378020.00 content_loss : 21965117440.00 style_loss : 15395375.00\n","iter : 121 total loss : 34873672.00 content_loss : 20508205056.00 style_loss : 14347908.00\n","iter : 136 total loss : 32795304.00 content_loss : 19296960512.00 style_loss : 13480750.00\n","training complete.\n","Final Image Shape = (512, 682, 3)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4h4hM7MBNQfk","executionInfo":{"elapsed":251,"status":"ok","timestamp":1636152873741,"user":{"displayName":"Vincent Young","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhMQp7rcKpD4q6aD-gNzxTzQGL5plg3iiAYpYj-=s64","userId":"15913363348651917887"},"user_tz":240},"outputId":"0bea9de6-f3d3-4b50-cecd-bd6dc74e86a3"},"source":["# Save image.\n","\n","save_name = content_image_name[:-4].replace('-','_') + '_' + style_image_name[:-4].replace('-','_')\n","print('Final Image name =', save_name)\n","\n","cv2.imwrite('{}.jpg'.format(save_name), cv2.cvtColor(generated_image, cv2.COLOR_RGB2BGR))"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Final Image name = content_image_style_image\n"]},{"data":{"text/plain":["True"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"hXi6ygfVNQqR","executionInfo":{"elapsed":296,"status":"ok","timestamp":1636152876836,"user":{"displayName":"Vincent Young","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhMQp7rcKpD4q6aD-gNzxTzQGL5plg3iiAYpYj-=s64","userId":"15913363348651917887"},"user_tz":240},"outputId":"ccbda70d-c363-45d5-bfa7-103e9ed4d4ce"},"source":["from google.colab import files\n","\n","files.download('{}.jpg'.format(save_name))"],"execution_count":null,"outputs":[{"data":{"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/javascript":["download(\"download_b4ab6e98-76b1-4953-b897-389061fb06af\", \"content_image_style_image.jpg\", 357535)"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"}]}]}